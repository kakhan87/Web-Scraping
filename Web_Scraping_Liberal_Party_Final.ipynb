{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Web_Scraping_Liberal_Party_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCxp-Qend5Z2",
        "colab_type": "text"
      },
      "source": [
        "## Web scraping with BeautifulSoup in Python - Liberals Blog\n",
        "- __Date__: July 25, 2020\n",
        "- __Author__: Karim Khan\n",
        "- __Description__: This Notebook shows scraping the Liberal Party of Canada's News Website by creating a function to scrape multiple number of posts over multiple pages (either x posts or posts since time t specified by the user)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjoModQsd5Z4",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HX9HjXVd5Z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8d9e75f-de6b-4a8f-99da-22620c372683"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "print(\"Libraries have been imported.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries have been imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv0HKApid5aA",
        "colab_type": "text"
      },
      "source": [
        "### Get top-level coverpage blogs for a single page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu4D237Cd5aC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://www.liberal.ca/blog/page/1/'\n",
        "coverpage_blogs = get(url, '{\"Accept-Language\": \"en-US, en;q=0.5\"}').content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbkjZMFPd5aH",
        "colab_type": "text"
      },
      "source": [
        "### Parse coverpage news with BS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EukYPkWSd5aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coverpage_soup = Soup(coverpage_blogs, 'lxml')\n",
        "# coverpage_soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7RNBp7xd5aN",
        "colab_type": "text"
      },
      "source": [
        "### Extract Element Tag containing essential information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz436piHd5aN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "020af90e-354a-4cb4-9d2a-3c31dc7dbc59"
      },
      "source": [
        "coverpage_news = coverpage_soup.find_all('div', class_ = 'cell large-4 medium-6 small-12')\n",
        "coverpage_news[0:2] #Display first 2 cards on coverpage "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<div class=\"cell large-4 medium-6 small-12\">\n",
              " <article class=\"post-listing-item\">\n",
              " <a class=\"post-listing-item__link\" href=\"https://www2.liberal.ca/fiscal-snapshot/\">\n",
              " <div class=\"post-listing-item__featured-image\" data-bg=\"https://s31184.pcdn.co/wp-content/uploads/sites/292/2020/07/fiscal-snapshot-1-EN-OG-1200x630-1-1024x538.jpg\"></div>\n",
              " <div class=\"post-listing-item__card-content\">\n",
              " <div class=\"post-listing-item__card-content-top\">\n",
              " <h3 class=\"post-listing-item__title-text text--red\">Snapshot: Supporting Canadians and stabilizing our economy</h3>\n",
              " <div class=\"post-listing-item__excerpt\">\n",
              " <p>Throughout this pandemic, Justin Trudeau and the Liberal government have been working hard to make sure you and your family are safe and supported during this challenging time.</p>\n",
              " </div>\n",
              " </div>\n",
              " <div class=\"post-listing-item__card-content-bottom\">\n",
              " <span class=\"button transparent-bg-button arrow-button\">\n",
              " \t\t\tRead More\t\t</span>\n",
              " </div>\n",
              " </div>\n",
              " </a>\n",
              " </article>\n",
              " </div>, <div class=\"cell large-4 medium-6 small-12\">\n",
              " <article class=\"post-listing-item\">\n",
              " <a class=\"post-listing-item__link\" href=\"https://www2.liberal.ca/justin-trudeaus-address-to-parliament-on-anti-black-racism-in-canada/\">\n",
              " <div class=\"post-listing-item__featured-image\" data-bg=\"https://s31184.pcdn.co/wp-content/uploads/sites/292/2020/07/racism-OG-1024x538.jpg\"></div>\n",
              " <div class=\"post-listing-item__card-content\">\n",
              " <div class=\"post-listing-item__card-content-top\">\n",
              " <h3 class=\"post-listing-item__title-text text--red\">Justin Trudeau’s address to Parliament on anti-Black racism in Canada</h3>\n",
              " <div class=\"post-listing-item__excerpt\">\n",
              " <p>We must stand together against racism, discrimination, and injustice so that we can build a better and more equitable Canada.</p>\n",
              " </div>\n",
              " </div>\n",
              " <div class=\"post-listing-item__card-content-bottom\">\n",
              " <span class=\"button transparent-bg-button arrow-button\">\n",
              " \t\t\tRead More\t\t</span>\n",
              " </div>\n",
              " </div>\n",
              " </a>\n",
              " </article>\n",
              " </div>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6V5puKad5aS",
        "colab_type": "text"
      },
      "source": [
        "### Test \"cleanliness\" of BeautifulSoup scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lLnf-k5d5aT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec4cf088-6e56-4f6f-f2e7-1381506467e7"
      },
      "source": [
        "print(coverpage_news[0].h3.get_text().strip()) #title\n",
        "print(coverpage_news[0].p.get_text()) #subtitle\n",
        "print(coverpage_news[0].a['href']) #link\n",
        "url = 'https://www.liberal.ca/justin-trudeaus-address-to-parliament-on-anti-black-racism-in-canada/'#example article\n",
        "article = get(url, headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}).content\n",
        "soup = Soup(article, 'lxml')\n",
        "body = soup.find('div', class_='post-content-container').get_text().strip()\n",
        "print(body) #body of example article"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Snapshot: Supporting Canadians and stabilizing our economy\n",
            "Throughout this pandemic, Justin Trudeau and the Liberal government have been working hard to make sure you and your family are safe and supported during this challenging time.\n",
            "https://www2.liberal.ca/fiscal-snapshot/\n",
            "Justin Trudeau’s address to Parliament on anti-Black racism in Canada\t\t\t\t\t\t\t\t\t\n",
            "\n",
            "\n",
            "\t\t\t\t\t\t\t\tJune 2, 2020\t\t\t\t\t\t\t\t\n",
            "\n",
            "\n",
            "\t\t\t\t\t\t\t\t\t\tShare\t\t\t\t\t\t\t\t\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Check against delivery\n",
            "June 2, 2020\n",
            "I rise today to address what so many people of colour live with every day.\n",
            "Over the past few days, we’ve seen horrific reports of police violence against Black men and women south of the border.\n",
            "But these are not isolated incidents or elsewhere problems.\n",
            "Prejudice, discrimination, and violence is a lived reality for far too many people.\n",
            "It is the result of systems which far too often condone, normalize, perpetrate, and perpetuate inequality and injustice against people of colour.\n",
            "As a country, we are not concerned bystanders simply watching what is happening next door.\n",
            "We are part of it.\n",
            "The calls for justice, for equality, for peace, have found echo in our communities because anti-Black racism is also happening here.\n",
            "Everywhere in Canada.\n",
            "Every single day. \n",
            "This is something that our own staff, Cabinet ministers, and colleagues face even in these halls.\n",
            "Over the past few days, I’ve heard many of these personal stories directly from them.\n",
            "And I’m not just talking about acts of violence. I’m also talking about microaggressions, which many of us may not even see.\n",
            "That is the daily reality of far too many racialized Canadians. And it needs to stop.\n",
            "When it comes to being an ally, I have made serious mistakes in the past – mistakes which I deeply regret, and continue to learn from.\n",
            "And I want to thank my colleagues, community leaders, and fellow Canadians for opening my eyes to what is really going on in our communities and helping me better understand both privilege and power.\n",
            "I’m not perfect.\n",
            "But not being perfect is not a free pass to not do the right thing.\n",
            "It’s not an excuse to not step up.\n",
            "To stand up for each other, to be an ally.\n",
            "I know that for so many people listening right now, the last thing you want to hear is another speech on racism from a white politician.\n",
            "I’m not here today to describe a reality I do not know or speak to a pain I have not felt. \n",
            "I’m here because I want you to know that our government is listening.\n",
            "We hear your calls for justice, equality, and accountability.\n",
            "We acknowledge your frustration, your anger, your heartbreak.\n",
            "We see you.\n",
            "Since taking office, our government has taken concrete action to fight anti-Black racism, systemic discrimination, and injustice across the country.\n",
            "We have worked with communities to recognize and address injustices.\n",
            "We’ve taken action to support community organizations, invest in better data, and fight racism.\n",
            "For example, we have provided $9 million to support programs for young Black Canadians.\n",
            "We have made significant investments to help the Public Health Agency of Canada provide more mental health services to those who have experienced racism or intergenerational trauma.\n",
            "We are helping community organizations obtain funding to buy equipment or rent space.\n",
            "And we have created the Anti-Racism Secretariat, which has a $4.6 million budget to eliminate systemic barriers that perpetuate injustice, notably in employment, justice, and social participation.\n",
            "And while we’ve made some progress, there is still so much more to do.\n",
            "Because here are the facts in Canada.\n",
            "Anti-Black racism is real.\n",
            "Unconscious bias is real.\n",
            "Systemic discrimination is real.\n",
            "For millions of Canadians, it is their daily, lived reality.\n",
            "The pain and damage it causes is real, too.  \n",
            "Mr. Speaker, every Canadian who has felt the weight of oppression, every student who has the courage to demand a better future, every person who marches and posts and reads and fights from Vancouver to Montréal to Halifax expects more than the status quo.\n",
            "They expect more and deserve better.\n",
            "The Government of Canada has a lot of work to do, but we are ready.\n",
            "We are ready to work with our opposition colleagues, community leaders, and Canadians to make our country fairer and more equal.\n",
            "Racism never has a place in our society.\n",
            "And we will do everything we can to eradicate it from coast to coast.\n",
            "Thank you, Mr. Speaker.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xVcKXP9d5aX",
        "colab_type": "text"
      },
      "source": [
        "### Helper Function 1 - Get X Posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QkeiGcqd5aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper Function 1 - Get X Posts\n",
        "def getNumPosts(number_of_posts = int):\n",
        "    \n",
        "    #create a blog counter\n",
        "    blog_count = 0\n",
        "           \n",
        "    url = 'https://www.liberal.ca/blog/'\n",
        "    coverpage_blogs = get(url, {\"Accept-Language\": \"en-US, en;q=0.5\"}).content\n",
        "    \n",
        "    #Parse coverpage news with BS\n",
        "    coverpage_soup = Soup(coverpage_blogs, 'lxml')\n",
        "    \n",
        "    # get total number of pages:\n",
        "    pages = [i.text for i in coverpage_soup.find_all('a',class_='page') if 'https://www2.liberal.ca/category/blog/page/' in str(i)]\n",
        "    total_pages = int(pages[-1])\n",
        "\n",
        "    filename = 'Liberals_Num_Posts.csv' #File name to store the scraped data\n",
        "    f = open(filename, 'w', encoding='utf-8')\n",
        "\n",
        "    #Defining header for csv\n",
        "    headers = 'Blog No., Title, Subtitle, Date, Link, Content\\n'\n",
        "    f.write(headers)\n",
        "    f.close()\n",
        "    print(\"New CSV file opened successfully\")\n",
        "    \n",
        "    for page in range(1, total_pages+1):\n",
        "\n",
        "        # Make a get request\n",
        "        url = ('https://www.liberal.ca/blog/page/' + str(page) + '/')\n",
        "        \n",
        "        coverpage_blogs = get(url, {\"Accept-Language\": \"en-US, en;q=0.5\"}).content\n",
        "\n",
        "        #Parse coverpage news with BS\n",
        "        coverpage_soup = Soup(coverpage_blogs, 'lxml')\n",
        "\n",
        "        coverpage_news = coverpage_soup.find_all('div', class_ = 'cell large-4 medium-6 small-12')\n",
        "               \n",
        "        for n in range(0, 9):\n",
        "            \n",
        "            # Getting the link of the article\n",
        "            try:\n",
        "                link = coverpage_news[n].a['href']\n",
        "                \n",
        "            except:\n",
        "                link = np.nan\n",
        "\n",
        "            # Getting the title\n",
        "            try:\n",
        "                title = coverpage_news[n].h3.get_text().strip()\n",
        "\n",
        "            except:\n",
        "                title = np.nan\n",
        "\n",
        "            # Getting the subtitle\n",
        "            try:\n",
        "                subtitle = coverpage_news[n].p.get_text().strip()\n",
        "\n",
        "            except:\n",
        "                subtitle = np.nan\n",
        "\n",
        "            # Reading the content (it is divided in paragraphs)\n",
        "            article = get(link, '{\"Accept-Language\": \"en-US, en;q=0.5\"}')\n",
        "            article_content = article.content\n",
        "            soup_article = Soup(article_content, 'lxml')\n",
        "\n",
        "            # Getting the date\n",
        "            try:\n",
        "                date = soup_article.find('p', class_='single__date').get_text()\n",
        "\n",
        "            except:\n",
        "                date = np.nan\n",
        "\n",
        "            # Getting the content\n",
        "\n",
        "            try:\n",
        "                body = soup_article.find('div', class_='post-content-container').get_text().strip()\n",
        "\n",
        "            except:\n",
        "                body = np.nan\n",
        "                          \n",
        "            blog_count += 1\n",
        "            \n",
        "            if blog_count > number_of_posts:\n",
        "                break\n",
        "                \n",
        "           # Append data sequentially to CSV file after each loop      \n",
        "            with open('Liberals_Num_Posts.csv', mode='a', encoding='utf-8') as csv_file:\n",
        "                fieldnames = ['Blog No.','Title','Subtitle','Date', 'Link', 'Content']\n",
        "                writer = csv.DictWriter(csv_file, fieldnames = fieldnames) \n",
        "\n",
        "                # Assign to each row\n",
        "                writer.writerow({'Blog No.':blog_count,'Title':title,'Subtitle':subtitle, 'Date':date,'Link':link, 'Content':body})\n",
        "\n",
        "                # print Success every time new row is added\n",
        "                print('Blog {} written successfully'.format(blog_count))\n",
        "                           \n",
        "        if blog_count > number_of_posts:\n",
        "            print(\"Provided number of posts reached\")\n",
        "            break\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkScflTAd5ac",
        "colab_type": "text"
      },
      "source": [
        "### Test Helper Function 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QskDnhPld5ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Commented out since it will be demonstrated in the Main Function below.\n",
        "# getNumPosts(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY3htc_Yd5ah",
        "colab_type": "text"
      },
      "source": [
        "### Helper Function 2 - Get Posts since time t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKNQN_XAd5ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper Function 2 - Get Posts since time t\n",
        "def getDatePosts():\n",
        "    \n",
        "    try:\n",
        "        date_since = input('Enter date to scrape posts from in [e.g. January 01, 2020]: ')\n",
        "        date_since_convert = datetime.datetime.strptime(date_since, '%B %d, %Y') # Converted date from string to datetime format\n",
        "\n",
        "    except:\n",
        "        print(\"Incorrect format, please try again\")\n",
        "        date_since = input('Enter date to scrape posts from in [e.g. January 01, 2020]: ')\n",
        "        date_since_convert = datetime.datetime.strptime(date_since, '%B %d, %Y') # Converted date from string to datetime format\n",
        "           \n",
        "    #create a blog counter\n",
        "    blog_count = 0\n",
        "       \n",
        "    # get total number of pages:\n",
        "    url = 'https://www.liberal.ca/blog/'\n",
        "    coverpage_blogs = get(url, '{\"Accept-Language\": \"en-US, en;q=0.5\"}').content\n",
        "    \n",
        "    #Parse coverpage news with BS\n",
        "    coverpage_soup = Soup(coverpage_blogs, 'lxml')\n",
        "    pages = [i.text for i in coverpage_soup.find_all('a',class_='page') if 'https://www2.liberal.ca/category/blog/page/' in str(i)]\n",
        "    total_pages = int(pages[-1])\n",
        "    \n",
        "    filename = 'Liberals_Dated_Posts.csv' #File name to store the scraped data\n",
        "    f = open(filename, 'w', encoding='utf-8')\n",
        "\n",
        "    #Defining header for csv\n",
        "    headers = 'Blog No., Title, Subtitle, Date, Link, Content\\n'\n",
        "    f.write(headers)\n",
        "    f.close()\n",
        "    print(\"New CSV file opened successfully\")\n",
        "\n",
        "    for page in range(1, total_pages+1):\n",
        "\n",
        "        # Make a get request\n",
        "        url = ('https://www.liberal.ca/blog/page/' + str(page) + '/')\n",
        "        \n",
        "        coverpage_blogs = get(url, '{\"Accept-Language\": \"en-US, en;q=0.5\"}').content\n",
        "\n",
        "        #Parse coverpage news with BS\n",
        "        coverpage_soup = Soup(coverpage_blogs, 'lxml')\n",
        "\n",
        "        coverpage_news = coverpage_soup.find_all('div', class_ = 'cell large-4 medium-6 small-12')\n",
        "                        \n",
        "        for n in range(0, 9):\n",
        "            \n",
        "            # Getting the link of the article\n",
        "            try:\n",
        "                link = coverpage_news[n].a['href']\n",
        "                \n",
        "            except:\n",
        "                link = np.nan\n",
        "\n",
        "            # Getting the title\n",
        "            try:\n",
        "                title = coverpage_news[n].h3.get_text().strip()\n",
        "            \n",
        "            except:\n",
        "                title = np.nan\n",
        "\n",
        "            # Getting the subtitle\n",
        "            try:\n",
        "                subtitle = coverpage_news[n].p.get_text().strip()\n",
        "            \n",
        "            except:\n",
        "                subtitle = np.nan\n",
        "\n",
        "            # Reading the content\n",
        "            article = get(link, '{\"Accept-Language\": \"en-US, en;q=0.5\"}')\n",
        "            article_content = article.content\n",
        "            soup_article = Soup(article_content, 'lxml')\n",
        "\n",
        "            # Getting the date\n",
        "            try:\n",
        "                date = soup_article.find('p', class_='single__date').get_text().strip()\n",
        "                date_convert = datetime.datetime.strptime(date, '%B %d, %Y') # Converted date from string to datetime format\n",
        "                \n",
        "            except:\n",
        "                date_convert = datetime.datetime.strptime(date, '%B %d, %Y')\n",
        "\n",
        "            # Getting the content\n",
        "            try:\n",
        "                body = soup_article.find('div', class_='post-content-container').get_text().strip()\n",
        "                \n",
        "            except:\n",
        "                body = np.nan\n",
        "\n",
        "            blog_count += 1\n",
        "            \n",
        "            if date_convert < date_since_convert:\n",
        "                break\n",
        "                           \n",
        "            # Append data sequentially to CSV file after each loop  \n",
        "\n",
        "            with open('Liberals_Dated_Posts.csv', mode='a', encoding='utf-8') as csv_file:\n",
        "                fieldnames = ['Blog No.','Title','Subtitle','Date', 'Link', 'Content']\n",
        "                writer = csv.DictWriter(csv_file, fieldnames = fieldnames) \n",
        "\n",
        "                # Assign to each row\n",
        "                writer.writerow({'Blog No.':blog_count,'Title':title,'Subtitle':subtitle, 'Date':date,'Link':link, 'Content':body})\n",
        "\n",
        "            # print success message every time new row is added\n",
        "            print('Blog {} written successfully'.format(blog_count))\n",
        "\n",
        "        if date_convert < date_since_convert:\n",
        "            print(\"Provided range of posts reached\")\n",
        "            break\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4pWUY0d5al",
        "colab_type": "text"
      },
      "source": [
        "### Test Helper Function 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUXF2OAbd5am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Commented out since it will be demonstrated in the Main Function below.\n",
        "# getDatePosts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjA8Lb9Od5ap",
        "colab_type": "text"
      },
      "source": [
        "### Main Function - Gives User the option to either scrape specific number of posts or posts since a specific date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq3Eaw_4d5aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main Function - Including Helper Functions 1 and 2.\n",
        "def WebScraping():\n",
        "    user_input = str(input(\"Enter Your Choice: \\n A) Scrape specific number of posts \\n B) Scrape posts since a specific date \\n \"))\n",
        "\n",
        "    if user_input == 'A':\n",
        "        number_of_posts = int(input(\"Enter number of blogs to be scraped (from most recent backwards): \"))\n",
        "        value = getNumPosts(int(number_of_posts))\n",
        "        return value\n",
        "        print(\"Data printed successfully!\")\n",
        "\n",
        "    elif user_input == 'B':\n",
        "        value = getDatePosts()\n",
        "        return value\n",
        "        print(\"Data printed successfully!\")\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid selection, please try again.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CFtS-Rwd5av",
        "colab_type": "text"
      },
      "source": [
        "### Test Main Function Works - X Posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdLHPpOcd5aw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "40567f64-42e6-43c5-d949-2aca2c92b0fd"
      },
      "source": [
        "WebScraping()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Your Choice: \n",
            " A) Scrape specific number of posts \n",
            " B) Scrape posts since a specific date \n",
            " A\n",
            "Enter number of blogs to be scraped (from most recent backwards): 10\n",
            "New CSV file opened successfully\n",
            "Blog 1 written successfully\n",
            "Blog 2 written successfully\n",
            "Blog 3 written successfully\n",
            "Blog 4 written successfully\n",
            "Blog 5 written successfully\n",
            "Blog 6 written successfully\n",
            "Blog 7 written successfully\n",
            "Blog 8 written successfully\n",
            "Blog 9 written successfully\n",
            "Blog 10 written successfully\n",
            "Provided number of posts reached\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Obln-qQd5a0",
        "colab_type": "text"
      },
      "source": [
        "### Test Main Function Works - Get Posts since time t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiNaVzmzd5a0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "cf55ed6a-5af3-4ad9-b14c-1acac0aa735f"
      },
      "source": [
        "WebScraping()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Your Choice: \n",
            " A) Scrape specific number of posts \n",
            " B) Scrape posts since a specific date \n",
            " B\n",
            "Enter date to scrape posts from in [e.g. January 01, 2020]: May 01, 2020\n",
            "New CSV file opened successfully\n",
            "Blog 1 written successfully\n",
            "Blog 2 written successfully\n",
            "Blog 3 written successfully\n",
            "Provided range of posts reached\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}